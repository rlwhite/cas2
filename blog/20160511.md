#20160511#

So far, we've coded the first *TestReporter* test, *TestReporterPrintsTestPlan*, and the code that it tests.  The test passes, so we'll continue with *TestReporterPrintsOKForPassedTest*.  Before we continue, however, it's worth mentioning how we tested the printing of *TestReporter::printPlan()* funtion.  Notice that the *TestReporter* constructor takes an *ostream* reference.  This reference is assigned to the *out_* data member.  *TestReporter* then prints all its output to this data memeber.  By passing a *stringstream* object into the *TestReporter* object we create in *TestReporterPrintsTestPlan::run()*, we are able to catch the output for comparison to what we expect to see.  Even though we don't print anything to the screen.

OOPS, I just noticed that I forgot to add the *TestReporter* tests to the ##TODO## list.  I'll do that now.

**TODO**
--------
* TestRunner tests:

>* <del>TestRunnerReportsNoFailuresWhenAllTestsPass</del>
>* TestRunnerReportsNoFailuresWhenAllTestsSkipped
>* TestRunnerReportsSkipWhenTestIsSkipped
>* TestRunnerReportsNoSkipsWhenAllTestsPass
>* TestRunnerReportsNoSkipsWhenAllTestsFail
>* <del>TestRunnerReportsNoFailuresWhenAllTestsPassOrSkip</del>
>* TestRunnerReportsNoSkipsWhenAllTestsPassOrFail

* TestReporter tests:
>* <del>TestReporterPrintsOKForPassedTest</del>
>* TestReporterPrintsTestPlan
>* TestReporterPrintsNOTOKForFailedTest
>* TestReporterPrintsSKIPPEDForSkippedTest

* <del>Split the code into multiple files</del>
* <del>Refactor *TestRunner::runTest()*.  Should *tryTest()* return the test result (PASS, FAIL, SKIP) ?</del>
* Create test logger/reporter.  How should we format the report?  Use TAP-style output?
* Implement various assert macros for test writers
* Implement test construction macros

...

##TestReporter##
So, first we'll add a new test, *TestReporterPrintsTestPlan*.

Done.

...

OK, so far we've got 2 tests for *TestReporter*: *TestReporterPrintsTestPlan* and *TestReporterPrintsOKForPassedTest*.  Both tests pass as the code they test has been implemented.  However, we have some duplicate code:

	void TestReporterPrintsTestPlan::run()
	{
		std::stringstream outStream;
		TestReporter reporter(outStream);
		
		reporter.printPlan(10);
		
		if("1..10\n" != outStream.str())
			throw TestCase::TestFailed();
	}

	...
	
	void TestReporterPrintsOKForPassedTest::run()
	{
		std::stringstream outStream;
		TestReporter reporter(outStream);
		
		reporter.printResult(TestResult::Passed,
			                 1,
							 "MyFakeTest");
	    
		if("OK 1 - MyFakeTest\n" != outStream.str())
			throw TestCase::TestFailed();
	}

We'll refactor that now by creating a *TestReporterTest* base class.  We'll move the *stringstream* object into that class as a protected member.

	Yea, I don't really like protected data, but I believe it to be OK for what we're doing.  Any thoughts?  Hmmm, have I ask that before?

We'll also move our *TestReporter* object to the base class as well.

...

All tests still pass.

Now to add the tests *TestReporterPrintsNOTOKForFailedTest* and *TestReporterPrintsSKIPPEDForSkippedTest*.

Q: Did I plan out the tests too far in advance?  They seemed obvious, but I realize that I might have tought too far ahead.  Oh well, continuing...

...

So now we have the following test cases complete (meaning the code has been implemented and the tests pass):
	* TestReporter tests:
	>* TestReporterPrintsOKForPassedTest
	>* TestReporterPrintsTestPlan
	>* TestReporterPrintsNOTOKForFailedTest
	>* TestReporterPrintsSKIPPEDForSkippedTest

We'll finish the *TestReporter* tests next time.  It looks like we need to create a test *TestReporterPrintsTestSummary*, and probably do a bit of refactoring along the way.

So, here's the valgrind report:

#valgrind#
	==14472== Memcheck, a memory error detector
	==14472== Copyright (C) 2002-2013, and GNU GPL'd, by Julian Seward et al.
	==14472== Using Valgrind-3.10.1 and LibVEX; rerun with -h for copyright info
	==14472== Command: ./casTest
	==14472== 
	Running 13 tests.
		1. PassingTestThrowsNoExceptions: OK
		2. FailingTestThrowsTestFailed: OK
		3. SkippedTestThrowsTestSkipped: OK
		Running 6 tests.
			1. PassingTest: OK
			2. PassingTest: OK
			3. PassingTest: OK
			4. PassingTest: OK
			5. PassingTest: OK
			6. PassingTest: OK
		All tests PASSED
		4. TestRunnerReportsNoFailuresWhenAllTestsPass: OK
		   Running 3 tests.
			   1. PassingTest: OK
			   2. FailingTest: NOT OK
			   3. SkippedTest: SKIPPED
		   One or more tests FAILED (One or more tests SKIPPED)
	    5. TestRunnerReportsAFailureWhenATestFails: OK
		Running 4 tests.
			1. PassingTest: OK
			2. PassingTest: OK
			3. SkippedTest: SKIPPED
			4. PassingTest: OK
		All tests PASSED (One or more tests SKIPPED)
		6. TestRunnerReportsNoFailuresWhenAllTestsPassOrSkip: OK
		7. TestResultToStringReturnsPassedForPassed: OK
		8. TestResultToStringReturnsFailedForFailed: OK
		9. TestResultToStringReturnsSkippedForSkipped: OK
		10. TestReporterPrintsTestPlan: OK
		11. TestReporterPrintsOKForPassedTest: OK
		12. TestReporterPrintsNOTOKForFailedTest: OK
		13. TestReporterPrintsSKIPPEDForSkippedTest: OK
	All tests PASSED
	==14472== 
	==14472== HEAP SUMMARY:
	==14472==     in use at exit: 0 bytes in 0 blocks
	==14472==   total heap usage: 122 allocs, 122 frees, 8,129 bytes allocated
	==14472== 
	==14472== All heap blocks were freed -- no leaks are possible
	==14472== 
	==14472== For counts of detected and suppressed errors, rerun with: -v
	==14472== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
