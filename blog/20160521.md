#20160521#
I've been thinking about a couple of things for the past few days.  First, I'm not sure that *casTest* should be embedded in the overall *cas* framework.  In has occurred to me that a test framework should probably be a stand alone thing.  It should probably have it's own make files, it's own tools that it uses, and of course deliverables that are dependent on as few other things as possible.  (I guess this is another form of reducing dependencies in order to make something more useful.  Or at least useful in more contexts.)  When I extracted the first version of *casTest* from the first, and abandonded, *cas*, my motiviation was just to be able to finish a piece of the project.  Now, I am under the belief that *casTest* should be independently deployable.  I'll add an item to my *TODO* list.

I've also been rethinking the idea of having a single tool that can be used to create a working test "suite" within a directory, run the tests within that "suite", and perhaps do other chores related to test writing and maintenance.  It seems to me that perhaps this could be a good thing after all.  I'll probably go back and forth with this, but it's something that I want to consider.  I guess there still could be multiple executables, one of which could execute any of the others.  Anyway, I'll also add a line to my *TODO* list.

...

I downloaded *Two Blue Cubes, Ltd*'s *Catch* test framework from [Phil Nash's *GitHub* repository](https://github.com/philsquared/Catch).  I've been using it at work for a couple of days.  It's pretty nice.  It's certainly very usable and easily incorporated into any project.  It's all in header files.  I've been using the single header file verson.  With that, just define one macro in your *.cpp* file, include *catch.hpp* and start writing tests.  It even supports a *Behavior Driven Develop* model.  My only complaint is that the compile times are a bit longer that I would have liked.  I'll look into pre-compiling the header, but there is quite a bit of template code that I imagin will have to be compiled anyway.  Still, we'll likely use the framework for the current project and worry about optimizing the builds as we go.

...

So, anyway, I still want to finish *casTest*.  If nothing else it will be a nice little exercise.  So, here's the current *TODO* list, with the additions mentioned above.  Of course, we'll keep updating it as we go.

**TODO**
--------
* TestRunner tests:

>* <del>PassingTestThrowsNoExceptions</del>
>* <del>FailingTestThrowsTestFailed</del>
>* <del>SkippedTestThrowsTestSkipped</del>
>* <del>TestRunnerReportsNoFailuresWhenAllTestsPass</del>
>* TestRunnerReportsNoFailuresWhenAllTestsSkipped
>* TestRunnerReportsSkipWhenTestIsSkipped
>* TestRunnerReportsNoSkipsWhenAllTestsPass
>* TestRunnerReportsNoSkipsWhenAllTestsFail
>* <del>TestRunnerReportsNoFailuresWhenAllTestsPassOrSkip</del>
>* TestRunnerReportsNoSkipsWhenAllTestsPassOrFail

* TestReporter tests:
>* <del>TestReporterPrintsOKForPassedTest</del>
>* <del>TestReporterPrintsTestPlan</del>
>* <del>TestReporterPrintsNOTOKForFailedTest</del>
>* <del>TestReporterPrintsSKIPPEDForSkippedTest</del>
>* <del>TestReporterPrintsTestSummary</del>

* <del>Split the code into multiple files</del>
* <del>Refactor *TestRunner::runTest()*.  Should *tryTest()* return the test result (PASS, FAIL, SKIP) ?</del>
* <del>Create test logger/reporter.  How should we format the report?  Use TAP-style output?</del>
* Implement various assert macros for test writers
* Implement test construction macros

* Rework the make system to be fully independent
* Consider whether or not *casTest* should be built into a single executable tool (rather than multiple tools).
* Consider replace *out_* member of *TestReporter* with something else.

...

Before I do anything else, I want to rename *testReporterTests.h/cpp* to *tapTestReporterTests.h/cpp* since we are actually testing the concrete *TapTestReporter* class there.  As it now stands, *TestReporter* is abstract and has no real functionality.  I don't think I really need any tests for it.  **Am I mistaken about that?**

After watching a talk by [Andrei Alexandrescu](http://erdani.com/), I think I also want to replace the *std::ostream&* member of that interface.  I'm not exactly sure what I'll replace it with, but it is something I will consider very seriously.  Of course this is about performance and of course I realize that I may be thinking about optimization prematurely.  But, his statements about IO streams were pretty convencing.  Yet another line for my *TODO* list.

...

All tests still pass.

...

I'm thinking that most of the other tests for *TestRunner* will pass without any additional work.  I'll write the tests anyway, but I think I'd like to start working on the macros for a bit.  One bug that I never fixed in *casTest 1* was that the source line number of the problem was not accurately reported.  I'd very much like to get it right this time.  After I have the macros, which I'll try to use TDD in developing, I'll replace the current checks in the current tests with the appropriate macro.  After that, we'll implement the other tests on our list.

...

As I started looking into how I might implement various macros to aid a test writer, I came to the conclusion that if a test fails, I probably should output some sort of explanation for the failure.  I don't believe this is strictly required, but if it's my test I'd like to know the following:
>source file and line number of the failure
>reason for the failure

The *TestCase::TestFailed* exception class currently doesn't support providing any explanatory text.  My first thought was that the explantory text should be reported following any detected failed test case.  Then, I was thinking that maybe it would be OK to report it before the result of the test.  I am a little conflicted about this.  The reason is simply because of the current way *TestRunner* is implemented.  I thought that having the calls to the *TestReporter* all located within *runTests()* made for a nice clean little function.  Now, it looks like I may have to move the call to report the test result to *tryTest()*, which is probably still OK, but somehow it seems a little messier.  Anyway, I'll play around with a bit and see what I think.  At the very least it looks like I need a function on *TestReporter* to output the explanatory text from the *FailedTest* exception.  In addition, of course if I add a function, I need to start by adding a test for the function first.  And, of course, other tests as necessary until the function is finished.

...

Hmmmm, after a little reading, I have a bit of thinking to do.  My original idea was to simply change *TestFailed* to derived from *std::runtime_error*, to take a string on construction and then output the string via *what()* once the exception was caught.  Now I'm not so sure that's the best approach.  It still might be where I end up, but I think I'll let this stew a while and do a little bit more reading.

Nothing much has changed, but, here's the valgrind report:

	==11287== Memcheck, a memory error detector
	==11287== Copyright (C) 2002-2013, and GNU GPL'd, by Julian Seward et al.
	==11287== Using Valgrind-3.10.1 and LibVEX; rerun with -h for copyright info
	==11287== Command: ./casTest
	==11287== 
	1..11
	ok 1 - PassingTestThrowsNoExceptions
	ok 2 - FailingTestThrowsTestFailed
	ok 3 - SkippedTestThrowsTestSkipped
	ok 4 - TestRunnerReportsNoFailuresWhenAllTestsPass
	ok 5 - TestRunnerReportsAFailureWhenATestFails
	ok 6 - TestRunnerReportsNoFailuresWhenAllTestsPassOrSkip
	ok 7 - TestReporterPrintsTestPlan
	ok 8 - TestReporterPrintsOKForPassedTest
	ok 9 - TestReporterPrintsNOTOKForFailedTest
	ok 10 - TestReporterPrintsSKIPPEDForSkippedTest
	ok 11 - TestReporterPrintsTestSummary
	Tests ran: 11
		FAILED: 0
		SKIPPED: 0
	==11287== 
	==11287== HEAP SUMMARY:
	==11287==     in use at exit: 0 bytes in 0 blocks
	==11287==   total heap usage: 96 allocs, 96 frees, 8,072 bytes allocated
	==11287== 
	==11287== All heap blocks were freed -- no leaks are possible
	==11287== 
	==11287== For counts of detected and suppressed errors, rerun with: -v
	==11287== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)





