20160423
========
I am faily satisfied with how the basic test case implementation has
gone.  I began with a single test case which always passed.  Of
course, to get there, I first added a line in *main()* to create and run
a test case object.  Then since that line wouldn;t compile, I added
the class, TestCase.  Then of course, I returned false from run to get
a proper failure.  Then I changed *run()* to return true.  At this point
casTest looked something like the following, I remember correctly.  I
don't remember when I added the convenience macro and *printResult()*
function, but they will eventually be replaced.

    #include <iostream>
	using namespace std;
	
    #define print(msg) cout << msg << endl
	
	bool printResult(bool result)
	{
		print("\t: " << (result ? "OK" : "NOT OK"));
		return result;
	}

	struct TestCase
	{
		bool run()
		{
			return true;
		}
	};

	int main(int argc, const char* const argv[])
	{
		int errCount(0);
		errCount += !printResult(TestCase().run());

	    print("" << (0 == errCount ? "All tests PASSED" : "One or more tests"));

        return errCount;
	}

Next, I added a failing test.  Of course, I needed a base class to
provide a common interface for running tests, so the current TestCase
class became, PassingTestCase, and I created a new TestCase abstract
base class with a pure virtual run() function.  Then I changed
PassingTestCase to derive from TestCase and re-ran the test.  All was
well.  That is it was OK for where I was in the process.  So, next I
added a line in main() to create and run a FailingTestCase object.

        ...
	    errCount += !printResult(PassingTestCase().run());
	    errCount += printTestResult(FailingTestCase().run());
		...

After the compile failure, implement, and correct process, I ended up
with both tests passing, even though due to the way main() counts the
errors, I had to negate the passing test case to get the err count
right.  Since the FailingTestCase returned false from run(), it's
return value was just used to increment the errCount.  Suffice it to
say that I wasn't at all satisfied with the way the code read.  After
some thought, I realized that I really wasn't testing the right thing
anyway.  Yes, I proved that I could run a test and determine of the
test had passed or failed, but I didn't have a way to test whether my
future test framework would be able to determine whether an arbitary
test case passed or failed.  I needed something else: a
PassingTestTest and a FailingTestTest.  Each of these would pass if
it detected the correct exit state of it's target test case.  After a
few more interations, I ended up where I left it.  I'll tag the
current files as 20160421.

Looking at what we have so far, we have an abstract base class,
TestCase, which defines the interface for all furture test cases.  We
have four TestCase derived classes; PassingTest, PassingTestTest,
FailingTest, and FailingTestTest.  We also have functions for creating
our tests, destroying a collection of tests, and for running a set of
tests.  These functions provide the basis for the behavior of our test
framework.

TestCase
--------

	struct TestCase
	{
		struct TestFailed
		{};

	    struct TestSkipped
		{};

	    virtual ~TestCase()
		{}

        virtual void run() = 0;

	    const std::string& name() const
        {
		    return name_;
		}

    protected:
		void setName(const std::string& name)
		{
			name_ = name;
		}

    private:
		std::string name_;
	};

Within this class we define two exceptions: *TestFailed* and
*TestSkipped*.  At this point, I'm thinking that these two exceptions
should be sufficient for all future needs.  I guess we'll see about
that.  I so believe these two exception classes should take us quite
far.  Again, we'll see.  :)

Of course, we provide a virtual destructor, and a pure virtual *run()*
function.  At this point, I'm toying around with the idea of having a
non-virtual interface in TestCase that will call the virtual *run()*.
I'm a bit undecided about it, so we'll leave that alone for now.  As
can be seen we have changed *run()* from being a function that
returned a bool to indicate success or failure to returning void.
Each test that completes without throwing one of the exceptions
described above will be considered as a passing test.  We'll be
providing a set of assert functions (probably wrapped in convenience
macros) to allow asserting conditions that will determine test success
or failure.  This is commonly done with testing frameworks, and is
certainly a good idea.  The first version of *casTest* (we'll call it
*casTest 1* from here on) provided a single assert function we
required the user to provide a string to tbe reported if the assert
failed.  We'll try to do better this time.

Each test should probably be identified by a name.  At the very least
this is necessary for reporting.  I thought about using typeid(), but
I think for now, we'll just have the derived classes call set name.  I
don't really like calling the protected *setName()*.  I would normally
prefer that base classes provide a constructor that derived classes
call for passing arguments.  However, this allows TestCase to remain
very simple, having only the compiler generated default constructor.
I'll likely revisit this in the future.  The plan is to provide a set
of macros to ease test writing very similar to the first version of
casTest, so the user will not explicitly write or even see the
implementation of the test case classes.  So, it probably doesn't
matter which of these mechanisms are used.  We'll leave it as is for
now.

All tests built and run as part of the casTest framework will derive
from *TestCase*.  We'll be moving it into a header file during our
next seesion.

PassingTest, FailingTest, PassingTestTest, FailingTestTest
----------------------------------------------------------
The four derived test cases, developed using *TDD*, provide a very
basic self-test capability.  We'll continue developing the framework
by using them as necessary.  Eventually, we'll likely move these
classes into a library that will be loaded and run as any other test
suite built using casTest.  For now, we'll just leave them in the main
source file.  As a reminder, it is our intention that all test suites
be built as shared libraries, that will be loaded and executed by the
*casTest* executable.

    struct PassingTest : TestCase
	{
		PassingTest()
		{
			setName("PassingTest");
		}

	    void run()
		{}
	};

	struct PassingTestTest : PassingTest
	{
		PassingTestTest()
		{
			setName("PassingTestTest");
		}

	    void run()
		{
			try
			{
				PassingTest::run();
			}
			catch(const TestCase::TestFailed& x)
			{
				throw;
			}
		}
	};

	struct FailingTest : TestCase
	{
		FailingTest()
		{
			setName("FailingTest");
		}

	    void run()
		{
			throw TestFailed();
		}
	};

	struct FailingTestTest : FailingTest
	{
		FailingTestTest()
		{
			setName("FailingTestTest");
		}

	    void run()
		{
			bool success(false);

	        try
			{
				FailingTest::run();
			}
			catch(const TestCase::TestFailed& x)
			{
				success = true;
			}

	        if(!success)
				throw TestCase::TestFailed();
	    }
	};

	struct SkippedTest : TestCase
	{
		SkippedTest()
		{
			setName("SkippedTest");
		}

	    void run()
		{
			throw TestSkipped();
		}
	};

	struct SkippedTestTest : SkippedTest
	{
		SkippedTestTest()
		{
			setName("SkippedTestTest");
		}

	    void run()
		{
		    bool success(false);

	        try
			{
				SkippedTest::run();
			}
			catch(const TestCase::TestSkipped& x)
			{
				success = true;
			}

	        if(!success)
				throw TestCase::TestFailed();
	    }
	};

runTests()
----------
The *runTests()* function will provide one of the main capabilities
for the *casTest* executable.  It simply iterates over a collection of
tests, calling each instance's *run()* function and prints the
results.  In it's current form, it also prints a header specifying the
number of tests that will be run, then a summary of the results (all
tests passed, or the number of failures).  The result reporting parts
of this function will likely be moved to a TestReport class of some
type.  This will likely be stream based, printing its output to
standard.  We may provide an option for output to be saved to file as
well, or instead.  *runTests()* could probably be placed into a test
runner class, but we'll defer that decision.  We will probably
refactor it a bit also to keep things clean.

	int runTests(std::vector<TestCase*>& tests)
	{
		print("Running " << tests.size() << " tests.");
		int errCount(0);

	    for(size_t t(0); t < tests.size(); ++t)
		{
			int oldErrCount(errCount);

	        try
			{
				tests[t]->run();
			}
			catch(const TestCase::TestFailed& x)
			{
				++errCount;
			}
	
	        print("\t" << (t + 1) << ": " << tests[t]->name() << ": " <<
				  (oldErrCount == errCount ? "OK" : "NOT OK"));
	    }
    
		printResult(errCount);

	    return errCount;
	}

createTests(), deleteTests()
----------------------------
The functions *createTests()* and *deleteTests()* will be provided by
the test suites themselves.  We'll try and provide a means for making
this an automated process so that it keeps the creation of test suites
as simple as possible.  In *casTest 1*, we actually required the test
writer to save his test suites to *.tpp* files.  I'd like to avoid
that this time.  Still, we'll need to scan the test source code and
generate the *createTests()* and *destroyTests()* functions.  I'm not
sure how we'll do that at this point, but I do have an idea along the
line of the following:

> We will provide a tool to add (create) a test suite.  This tool will
> generate a test source file, alomg with a makefile for the test
> suite.  In the test suite makefile, the test sources will be
> assigned to a make variable (something like TESTSRCS).  From the
> files in TESTSRCS, we'll generate an additional source file that
> will generate *createTests()*, adding a dynamically created test
> case instance to the test collection for each test defined in the
> test sourec files.  It will generate a *destroyTests()* function
> that simply iterates over every test in the collection and deletes
> it.  Of course, this is a preliminary thought, but is similar to
> what we did in *casTest 1*.

valgrind
--------
Of course it is our goal to provide a high quality product.  This is
especially important given the nature of unit testing.  The last thing
anyone needs is for a flaw in their unit test framework to cause a test
to pass or fail erroneously.  Here's what valgrind has to say about
the current state of *casTest 2*.  Yea, I know, there's not much there
yet.  Still, it's always nice to see a report like that....yes?

>==15641== Memcheck, a memory error detector
>
>==15641== Copyright (C) 2002-2013, and GNU GPL'd, by Julian Seward et al.
>
>==15641== Using Valgrind-3.10.1 and LibVEX; rerun with -h for copyright info
>
>==15641== Command: ./casTest
>
>==15641==
>
>Running 3 tests.
>
>>	1: PassingTestTest: OK
>
>>	2: FailingTestTest: OK
>
>>	3: SkippedTestTest: OK
>
>All tests PASSED
>
>==15641==
>
>==15641== HEAP SUMMARY:
>
>==15641==     in use at exit: 0 bytes in 0 blocks
>
>==15641==   total heap usage: 14 allocs, 14 frees, 590 bytes
>allocated
>
>==15641==
>
>==15641== All heap blocks were freed -- no leaks are possible
>
>==15641==
>
>==15641== For counts of detected and suppressed errors, rerun with:
>-v
>
>==15641== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from
>0)
>
