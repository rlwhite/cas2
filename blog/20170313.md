##20170313##

Well, it's been a while since I've worked on this, or even looked at it.  Looking at the blog for 20160901, my *TODO* list looked looks like this:

---
**TODO (20160901)**
--------
* TestRunner tests:

>* <del>PassingTestThrowsNoExceptions</del>
>* <del>FailingTestThrowsTestFailed</del>
>* <del>SkippedTestThrowsTestSkipped</del>
>* <del>TestRunnerReportsNoFailuresWhenAllTestsPass</del>
>* <del>TestRunnerReportsNoFailuresWhenAllTestsSkipped</del>
>* <del>TestRunnerReportsSkipWhenTestIsSkipped</del>
>* <del>TestRunnerReportsNoSkipsWhenAllTestsPass</del>
>* <del>TestRunnerReportsNoSkipsWhenAllTestsFail</del>
>* <del>TestRunnerReportsNoFailuresWhenAllTestsPassOrSkip</del>
>* <del>TestRunnerReportsNoSkipsWhenAllTestsPassOrFail</del>

* TestReporter tests:
>* <del>TestReporterPrintsOKForPassedTest</del>
>* <del>TestReporterPrintsTestPlan</del>
>* <del>TestReporterPrintsNOTOKForFailedTest</del>
>* <del>TestReporterPrintsSKIPPEDForSkippedTest</del>
>* <del>TestReporterPrintsTestSummary</del>

* <del>Split the code into multiple files</del>
* <del>Refactor *TestRunner::runTest()*.  Should *tryTest()* return the test result (PASS, FAIL, SKIP) ?</del>
* <del>Create test logger/reporter.  How should we format the report?  Use TAP-style output?</del>

* Implement various assert macros for test writers
>* <del>EXPECT_TRUE</del>
>* <del>EXPECT_FALSE</del>
>* <del>EXPECT_THROWS</del>
>* <del>EXPECT_NOTHROW</del>
>* <del>EXPECT_EQ</del>
>* <del>EXPECT_NE<del>
>* <del>EXPECT_LT</del>
>* <del>EXPECT_LE</del>
>* <del>EXPECT_GT</del>
>* <del>EXPECT_GE<del>

>* <del>Create an error message factory that translates exception objects into
error messages</del>

* Implement test construction macros

* Rework the make system to be fully independent
* Consider whether or not *casTest* should be built into a single executable tool (rather than multiple tools).
* Consider replacing *out_* member of *TestReporter* with something else.

* <b>Prepare for moving self-tests to library by moving the following functions to a new file:
>* createTests()
>* deleteTests()</b>


* Create a library loader/plugin class to wrap the dynamic library routines we need for the plugins.
>* *void createTests(std::vector<TestCase*>&)*
>* *void deleteTests(std::vector<TestCase*>&)*
>* *TestReporter\* createTestReporter()*
>* *void deleteTestReporter(TestReporter\*)*

* Move all self-tests to a shared library so that the self test functions like any other test to be executed.

---

I would imagine that I had planned to start on the test construction macros next, but I think I'd like to work out a set of commands.  I don't image all that many are needed, but this list should be a good start:

    > run [test1.tst, test2.test, ..., testN.test]
	    >> run with no arguments means to attempt to run all .test files
		
	> addTest <nameOfTestSuite1>
	    >> We use the term "test suite" a little loosely in that we have no test suite class.  All tests within a file constitutes a suite of tests.
		>> If a test suite already exists in the working directory, an error message will be reported.
		
I guess those two commands will be a sufficient start.  Of course, we'll begin by writing a test.  We'll implement rhe "run" command first....We'll after dinner.

...


OK, so, all tests still run.  Of course they do, we haven't done anything yet.  As I am thinking about this, it seems that the run command will actually require quite a lot.  It will have to load the library that contains the tests (the *.test* file), then call createTests(), create a TestRunner and call runTests, then, finally call deleteTests()....//to be continued



		
	
